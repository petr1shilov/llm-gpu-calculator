import streamlit as st
import math

GPUS = {
    "T4": {"vram": 16, "fp16_tflops": 65, "supports_fp8": False, "base_tps_8b_fp16": 20},
    "A10": {"vram": 24, "fp16_tflops": 125, "supports_fp8": True, "base_tps_8b_fp16": 42},
    "A100_40G": {"vram": 40, "fp16_tflops": 312, "supports_fp8": True, "base_tps_8b_fp16": 130},
    "A100_80G": {"vram": 80, "fp16_tflops": 312, "supports_fp8": True, "base_tps_8b_fp16": 135},
    "H100_80G": {"vram": 80, "fp16_tflops": 1517, "supports_fp8": True, "base_tps_8b_fp16": 200},
    "L40": {"vram": 48, "fp16_tflops": 362, "supports_fp8": True, "base_tps_8b_fp16": 150},
    "RTX_4090": {"vram": 24, "fp16_tflops": 330, "supports_fp8": False, "base_tps_8b_fp16": 100},
    "RTX_3090": {"vram": 24, "fp16_tflops": 285, "supports_fp8": False, "base_tps_8b_fp16": 80},
    "L4": {"vram": 24, "fp16_tflops": 120, "supports_fp8": True, "base_tps_8b_fp16": 40}
}

PRECISION_BYTES = {
    "FP32": 4,
    "FP16": 2,
    "FP8": 1,
    "INT8": 1,
    "INT4": 0.5
}

def calculate_memory(parameters_b, precision_bytes, batch_size, context_length, num_layers):
    model_memory = (parameters_b * 1e9 * precision_bytes) / (1024 ** 3)
    kv_cache_memory = (batch_size * context_length * num_layers * 2 * precision_bytes) / (1024 ** 3)
    overhead = model_memory * 0.2
    total_memory = model_memory + kv_cache_memory + overhead
    return total_memory, model_memory, kv_cache_memory

def estimate_token_cost(price_per_hour, tps):
    if price_per_hour == 0 or tps == 0:
        return 0.0
    return round(price_per_hour / (tps * 3600), 10)

def estimate_efficiency(tps, price_per_hour):
    if price_per_hour == 0:
        return 0.0
    return round(tps / price_per_hour, 2)

def estimate_tps_for_gpu(gpu, parameters_b, precision, base_tps_8b_fp16):
    scale_factor = 8 / parameters_b
    tps = base_tps_8b_fp16 * scale_factor
    if precision in ["FP8", "INT8"]:
        tps *= 1.5
    elif precision == "INT4":
        tps *= 2.0
    elif precision == "FP32":
        tps *= 0.5
    return tps

def recommend_gpu(total_memory, precision, required_tps, parameters_b):
    recommendations = []
    for gpu, specs in GPUS.items():
        if total_memory > specs["vram"]:
            continue
        if precision == "FP8" and not specs["supports_fp8"]:
            continue
        estimated_tps = estimate_tps_for_gpu(gpu, parameters_b, precision, specs["base_tps_8b_fp16"])
        if estimated_tps >= required_tps:
            recommendations.append((gpu, estimated_tps, specs["vram"]))
    recommendations.sort(key=lambda x: x[1], reverse=True)
    return recommendations

def suggest_multi_gpu(total_memory, min_tps, precision, parameters_b):
    options = []
    for gpu, specs in GPUS.items():
        if precision == "FP8" and not specs["supports_fp8"]:
            continue
        tps = estimate_tps_for_gpu(gpu, parameters_b, precision, specs["base_tps_8b_fp16"])
        gpus_needed = math.ceil(total_memory / specs["vram"])
        if tps * gpus_needed >= min_tps:
            options.append((gpu, gpus_needed, tps * gpus_needed))
    return options

def estimate_latency(max_traffic, mean_tokens_per_query, tps):
    return (max_traffic * mean_tokens_per_query) / (tps * 8)

st.set_page_config(page_title="GPU –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –¥–ª—è LLM", page_icon="üß†")

st.title("üìä –ö–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –æ—Ü–µ–Ω–∫–∏ GPU –¥–ª—è LLM")
st.markdown("""
–≠—Ç–æ—Ç –∫–∞–ª—å–∫—É–ª—è—Ç–æ—Ä –ø–æ–º–æ–≥–∞–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å:

- üì¶ —Å–∫–æ–ª—å–∫–æ VRAM –Ω—É–∂–Ω–æ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–∏,
- üöÄ –ø—Ä–∏–º–µ—Ä–Ω—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ (TPS),
- üíµ —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–¥–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞,
- üìà —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (TPS/–≤–∞—Ä—é—Ç–∞),
- ü§ñ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤—ã–±–æ—Ä—É GPU –∏–ª–∏ multi-GPU, –µ—Å–ª–∏ –æ–¥–Ω–æ–π –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç.
""")

with st.sidebar:
    st.header("üß† –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏")
    st.markdown("""
    - **–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏** ‚Äî –≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, 7 –¥–ª—è LLaMA-3 7B).
    - **Batch size** ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    - **Sequence length** ‚Äî –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ —Ç–æ–∫–µ–Ω–∞—Ö.
    - **Hidden dimension** ‚Äî —Ä–∞–∑–º–µ—Ä —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –º–æ–¥–µ–ª–∏.
    - **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤** ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –±–ª–æ–∫–æ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.
    - **–¢–æ—á–Ω–æ—Å—Ç—å** ‚Äî –≤–ª–∏—è–µ—Ç –Ω–∞ –æ–±—ä—ë–º –ø–∞–º—è—Ç–∏ –∏ TPS.
    """)
    model_params_billion = st.number_input("–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (–≤ –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö)", value=7, step=1)
    batch_size = st.number_input("Batch size", value=4, step=1)
    seq_len = st.number_input("Sequence length", value=1024, step=128)
    hidden_dim = st.number_input("Hidden dimension", value=4096, step=256)
    n_layers = st.number_input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤", value=32, step=1)
    precision = st.selectbox("–¢–æ—á–Ω–æ—Å—Ç—å", options=["FP32", "FP16", "FP8", "INT8", "INT4"], index=1)
    dtype_bytes = PRECISION_BYTES[precision]

    st.header("üõ† –ü–∞—Ä–º–µ—Ç—Ä—ã –¥–ª—è –ø—Ä–∏–º–µ—Ä–Ω—ã—Ö —Ç–µ—Å—Ç–æ–≤")
    st.markdown("""
    - *–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ –∑–∞ —Ä–∞–∑* ‚Äî –ø–∏–∫–æ–≤—ã–π –∑–∞–≥—Ä—É–∑ —Å–∞–π—Ç–∞.
    - *–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ* ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ(–≤ —Å—Ç—Ä–µ–¥–Ω–µ–º).
    - *maximum latency* - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–∏ (—Å–µ–∫)
    """)
    max_traffic = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ç—Ä–∞—Ñ–∏–∫ –∑–∞ —Ä–∞–∑", value=500, step=10)
    mean_tokens_per_query = st.number_input("–°—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∞–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º –∑–∞–ø—Ä–æ—Å–µ", value=100, step=10)
    max_latency = st.number_input("maximum latency", value=7, step=1)

    st.header("üíª –ü–∞—Ä–∞–º–µ—Ç—Ä—ã GPU")
    currency = st.selectbox("–í–∞–ª—é—Ç–∞ –≤–≤–æ–¥–∞", ["‚ÇΩ", "$"], index=0)
    if currency == "$":
        usd_to_rub = st.number_input("–ö—É—Ä—Å USD‚ÜíRUB", value=90.0, step=1.0)

    st.markdown("""
    - *TFLOPS GPU* ‚Äî –ø–∏–∫–æ–≤–∞—è –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å –∫–∞—Ä—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, A10 ‚âà 150 TFLOPS).
    - *–°—Ç–æ–∏–º–æ—Å—Ç—å –∞—Ä–µ–Ω–¥—ã* ‚Äî —Ü–µ–Ω–∞ –∞—Ä–µ–Ω–¥—ã GPU –∑–∞ —á–∞—Å (–≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–π –≤–∞–ª—é—Ç–µ).
    """)
    base_tflops = st.number_input("TFLOPS GPU", value=150.0, step=10.0)
    if currency == "‚ÇΩ":
        price_per_hour = st.number_input("–°—Ç–æ–∏–º–æ—Å—Ç—å –∞—Ä–µ–Ω–¥—ã GPU (‚ÇΩ/—á–∞—Å)", value=90.0, step=1.0)
    else:
        price_per_hour_usd = st.number_input("–°—Ç–æ–∏–º–æ—Å—Ç—å –∞—Ä–µ–Ω–¥—ã GPU ($/—á–∞—Å)", value=0.9, step=0.1)
        # usd_to_rub = st.number_input("–ö—É—Ä—Å USD‚ÜíRUB", value=90.0, step=1.0)
        price_per_hour = price_per_hour_usd * usd_to_rub
        st.caption(f"–ü–µ—Ä–µ—Å—á—ë—Ç: {price_per_hour_usd} $/—á–∞—Å √ó {usd_to_rub} = {price_per_hour:.2f} ‚ÇΩ/—á–∞—Å")
    required_tps = st.number_input("–ñ–µ–ª–∞–µ–º—ã–π TPS", value=50.0, step=1.0)

if st.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å"):
    total_mem, model_mem, kv_mem = calculate_memory(model_params_billion, dtype_bytes, batch_size, seq_len, n_layers)
    tps = estimate_tps_for_gpu("custom", model_params_billion, precision, base_tflops)
    cost = estimate_token_cost(price_per_hour, tps)
    efficiency = estimate_efficiency(tps, price_per_hour)
    latency = estimate_latency(max_traffic, mean_tokens_per_query, tps)

    st.subheader("üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã")
    st.metric("–í–µ—Å –º–æ–¥–µ–ª–∏ (GB)", round(model_mem, 2))
    st.metric("–û–±—ä—ë–º KV-–∫—ç—à–∞ (GB)", round(kv_mem, 2))
    st.metric("–û–±—â–∏–π –æ–±—ä—ë–º VRAM (GB)", round(total_mem, 2))
    st.metric("–û—Ü–µ–Ω–∫–∞ TPS", round(tps, 2))
    st.metric("–°—Ç–æ–∏–º–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–∞ (‚ÇΩ)", f"{cost:.8f} ‚ÇΩ")
    st.metric("–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (TPS/‚ÇΩ)", efficiency)
    st.metric("–°–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ (—Å–µ–∫)", round(latency, 2))

    st.subheader("üß† –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ GPU")
    recs = recommend_gpu(total_mem, precision, required_tps, model_params_billion)
    if recs:
        for gpu_name, est_tps, gpu_vram in recs:
            st.markdown(f"‚úÖ **{gpu_name}** ‚Äî TPS ‚âà `{round(est_tps)}`, VRAM: `{gpu_vram} GB`")
    else:
        st.warning("‚ùå –ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ GPU. –ù–∏–∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω—ã multi-GPU –≤–∞—Ä–∏–∞–Ω—Ç—ã.")
        multi_gpu = suggest_multi_gpu(total_mem, required_tps, precision, model_params_billion)
        if multi_gpu:
            st.subheader("üí° Multi-GPU –í–∞—Ä–∏–∞–Ω—Ç—ã")
            for gpu_name, num_gpus, total_tps in multi_gpu:
                st.markdown(f"- {num_gpus}√ó **{gpu_name}** ‚Äî –û–±—â–∏–π TPS ‚âà `{round(total_tps)}`")
        else:
            st.error("–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –¥–æ—Å—Ç–∏—á—å —Ü–µ–ª–∏ –¥–∞–∂–µ —Å multi-GPU. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å batch/context –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ.")
